{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# **Multiclass Classification using Keras and TensorFlow on Food-101 Dataset**\n![alt text](https://www.vision.ee.ethz.ch/datasets_extra/food-101/static/img/food-101.jpg)",
   "metadata": {
    "_uuid": "41a6777d5e67dc652f57ce9681b4c44dc44152be",
    "id": "uNaVQGQ9tQRr"
   }
  },
  {
   "cell_type": "code",
   "source": "\nimport requests\nimport tensorflow as tf\nimport matplotlib.image as img\n%matplotlib inline\nimport numpy as np\nfrom collections import defaultdict\nimport collections\nfrom shutil import copy\nfrom shutil import copytree, rmtree\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nimport cv2\nprint (\"all imported\")",
   "metadata": {
    "_uuid": "dd82702380162e9587a0eae2f644dae2764f93c8",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check if GPU is enabled\nprint(tf.__version__)\nprint(tf.test.gpu_device_name())\napi_url = 'https://api.calorieninjas.com/v1/nutrition?query='\nquery = 'french fries'\nresponse = requests.get(api_url + query, headers={'X-Api-Key': 's9S3fiY9BWYZtpCebS71Rg==Qk1nbauJKhKE9k8R'})\nif response.status_code == requests.codes.ok:\n    print(response.text)\nelse:\n    print(\"Error:\", response.status_code, response.text)\n",
   "metadata": {
    "_uuid": "70f06e9a535b5f32ad9d927fc00e767dd72f17dd",
    "id": "JOZZbCDoP-Hy",
    "outputId": "99f6277e-0b8e-4541-a9b4-cce1a98f5b57",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%cd /kaggle/input/food-101/",
   "metadata": {
    "_uuid": "c134497b2239d3fa5f377fb5aebf1887f627233f",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Helper function to download data and extract\ndef get_data_extract():\n  if \"food-101\" in os.listdir():\n    print(\"Dataset already exists\")\n  else:\n    print(\"Downloading the data...\")\n    !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n    print(\"Dataset downloaded!\")\n    print(\"Extracting data..\")\n    !tar xzvf food-101.tar.gz\n    print(\"Extraction done!\")",
   "metadata": {
    "_uuid": "339ff95b36c15cfb16a9316d2a19cc89aaaf7160",
    "id": "f88XvEBTQBS9",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "* **Commented the below cell as the Food-101 dataset is available from Kaggle Datasets and need not be downloaded..**",
   "metadata": {
    "_uuid": "c36f8e6e00be58a643a598469cc3fd9a8c47a912",
    "id": "gwLp2G9ae9xC"
   }
  },
  {
   "cell_type": "code",
   "source": "# Download data and extract it to folder\n# Uncomment this below line if you are on Colab\n\n#get_data_extract()",
   "metadata": {
    "_uuid": "05fd6dae2dde446b3bd4a17f81a202391f9328d3",
    "id": "O7kY0v23QJGO",
    "outputId": "edc14855-bf55-4938-a875-af7d24729ea6",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Understand dataset structure and files**",
   "metadata": {
    "_uuid": "5f65273b9b9b58da245745c82ac6cc5dfa54eeda",
    "id": "eQr6hmptQe6q"
   }
  },
  {
   "cell_type": "code",
   "source": "# Check the extracted dataset folder\n!ls food-101/",
   "metadata": {
    "_uuid": "7ceb7f04d55f3d1510a03868713e773e7df17046",
    "id": "7wJ_OH1DQyrd",
    "outputId": "bda2768c-24b9-4962-b7eb-fb3b07d6fad4",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**images** folder contains 101 folders with 1000 images  each  \nEach folder contains images of a specific food class",
   "metadata": {
    "_uuid": "d831522d781ad9711f592c3d33de4c58a02a9a36",
    "id": "9M2OZ8O_RVhu"
   }
  },
  {
   "cell_type": "code",
   "source": "os.listdir('food-101/images')",
   "metadata": {
    "_uuid": "9a3e091f8d5db4be5a553a2fd23970dde7c649f2",
    "id": "yy_pAK35Rbdi",
    "outputId": "9b374f07-961a-4878-85a2-86589b2f68cb",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "os.listdir('food-101/meta')",
   "metadata": {
    "_uuid": "c3f4490c55a470c25cf5f92f131f21701782e645",
    "id": "jdIDm6tkSwqY",
    "outputId": "8a3b9226-69ff-4bba-89fc-b303d1e10d7f",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!head food-101/meta/train.txt",
   "metadata": {
    "_uuid": "d938790622795157c456e6411999c571ccf64abe",
    "id": "55WJA5RTQtuL",
    "outputId": "d0302ba3-7d58-43c3-bd13-93b1696a9749",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!head food-101/meta/classes.txt",
   "metadata": {
    "_uuid": "2f606b4a970498cf9303c2d393605494b51bc3f6",
    "id": "a3yfov0gQocW",
    "outputId": "8ee3f518-986e-4135-e987-c61cd9b5b53c",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Visualize random image from each of the 101 classes**",
   "metadata": {
    "_uuid": "4af37d46992f3b2aa7b74efcb5102751b167914e",
    "id": "motIEZu_TVih"
   }
  },
  {
   "cell_type": "code",
   "source": "# Visualize the data, showing one image per class from 101 classes\nrows = 17\ncols = 6\nfig, ax = plt.subplots(rows, cols, figsize=(25,25))\nfig.suptitle(\"Showing one random image from each class\", y=1.05, fontsize=24) # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\ndata_dir = \"food-101/images/\"\nfoods_sorted = sorted(os.listdir(data_dir))\nfood_id = 0\nfor i in range(rows):\n  for j in range(cols):\n    try:\n      food_selected = foods_sorted[food_id] \n      food_id += 1\n    except:\n      break\n    if food_selected == '.DS_Store':\n        continue\n    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n    ax[i][j].imshow(img)\n    ax[i][j].set_title(food_selected, pad = 10)\n    \nplt.setp(ax, xticks=[],yticks=[])\nplt.tight_layout()\n# https://matplotlib.org/users/tight_layout_guide.html\n",
   "metadata": {
    "_uuid": "da6910e8bb064b76c17e07f0a2e0e23ebdefbbfa",
    "id": "Jfif27Pr5KEn",
    "outputId": "a451ddd4-2beb-4d36-eb00-000ed2f23286",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Split the image data into train and test using train.txt and test.txt**",
   "metadata": {
    "_uuid": "7a8300cde1146c50e672079b44346e723d813702",
    "id": "KIgareCETmct"
   }
  },
  {
   "cell_type": "code",
   "source": "# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src,dest):\n  classes_images = defaultdict(list)\n  with open(filepath, 'r') as txt:\n      paths = [read.strip() for read in txt.readlines()]\n      for p in paths:\n        food = p.split('/')\n        classes_images[food[0]].append(food[1] + '.jpg')\n\n  for food in classes_images.keys():\n    print(\"\\nCopying images into \",food)\n    if not os.path.exists(os.path.join(dest,food)):\n      os.makedirs(os.path.join(dest,food))\n    for i in classes_images[food]:\n      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n  print(\"Copying Done!\")",
   "metadata": {
    "_uuid": "d6c87e52c1af18e3243f1cb72b5834941828b286",
    "id": "xB0XMUX_5KMQ",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n%cd /\nprint(\"Creating train data...\")\nprepare_data('/kaggle/input/food-101/food-101/meta/train.txt', '/kaggle/input/food-101/food-101/images', 'train')",
   "metadata": {
    "_uuid": "786cdb3d60ebcdddb4c2875f9e0b4508c5210fc1",
    "id": "LSgcYcqy5KUd",
    "outputId": "7e65498b-bcd8-4209-87e8-bdc1a8c37b4e",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\nprint(\"Creating test data...\")\nprepare_data('/kaggle/input/food-101/food-101/meta/test.txt', '/kaggle/input/food-101/food-101/images', 'test')",
   "metadata": {
    "_uuid": "ba85577e0ef10c768e000ecf32dee36566d52599",
    "id": "JI65wZgT5Kb-",
    "outputId": "2e71e6bc-43de-4ea3-f5d6-a660c4a3dc42",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check how many files are in the train folder\nprint(\"Total number of samples in train folder\")\n!find train -type d -or -type f -printf '.' | wc -c",
   "metadata": {
    "_uuid": "b5a96c42b2af54aa994b86a58d2657b81786381b",
    "id": "Xccc8PJP5K1G",
    "outputId": "981ab583-491f-41ff-a128-d1f29c137775",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check how many files are in the test folder\nprint(\"Total number of samples in test folder\")\n!find test -type d -or -type f -printf '.' | wc -c",
   "metadata": {
    "_uuid": "492bb30a200d6cde501fbe2a6424c1c05a67c2e8",
    "id": "Iz3fjQw25K3-",
    "outputId": "b667062f-9acb-4be7-81ff-15347abdc750",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**",
   "metadata": {
    "_uuid": "b7ee945bd17a45706d665517a51f5adc1b6b9106",
    "id": "O5rLWIHpUGWf"
   }
  },
  {
   "cell_type": "code",
   "source": "# List of all 101 types of foods(sorted alphabetically)\ndel foods_sorted[0] # remove .DS_Store from the list",
   "metadata": {
    "_uuid": "27bf6dfb6b5c0a01efaad5de5ac90301663e84ca",
    "id": "b9i8vGHYKO-g",
    "outputId": "b810d29e-bdad-4aa1-cc66-3737c075408f",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "foods_sorted",
   "metadata": {
    "_uuid": "61dfaaf6466e5677c0c732c9133561e27d048ef0",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Helper method to create train_mini and test_mini data samples\ndef dataset_mini(food_list, src, dest):\n  if os.path.exists(dest):\n    rmtree(dest) # removing dataset_mini(if it already exists) folders so that we will have only the classes that we want\n  os.makedirs(dest)\n  for food_item in food_list :\n    print(\"Copying images into\",food_item)\n    copytree(os.path.join(src,food_item), os.path.join(dest,food_item))\n      ",
   "metadata": {
    "_uuid": "6101991b0ff433a76afdc2cb99c021862c83860e",
    "id": "tYyJGTJ6J9CP",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# picking 3 food items and generating separate data folders for the same\nfood_list = ['apple_pie','pizza','omelette']\nsrc_train = 'train'\ndest_train = 'train_mini'\nsrc_test = 'test'\ndest_test = 'test_mini'",
   "metadata": {
    "_uuid": "71c58df71a49e4e0e2e0bfe6359602c92c2f2306",
    "id": "9YAscZLV5LFK",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Creating train data folder with new classes\")\ndataset_mini(food_list, src_train, dest_train)",
   "metadata": {
    "_uuid": "2a6d67301fb0a709613c1c7a3f00629c349065e3",
    "id": "tvlXbJ3NoPzy",
    "outputId": "7a4173c9-9328-421e-e65f-0a25ecf55591",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Total number of samples in train folder\")\n\n!find train_mini -type d -or -type f -printf '.' | wc -c",
   "metadata": {
    "_uuid": "274b3f850418d859fdc0cffbbcbf8d33bdd8789b",
    "id": "t7mWJCev5LI8",
    "outputId": "76eab9e3-ca12-4cda-a551-e2629d0cb06e",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Creating test data folder with new classes\")\ndataset_mini(food_list, src_test, dest_test)",
   "metadata": {
    "_uuid": "93fb0d0f94b56dd488ebe3e4964b9799cf5228b3",
    "id": "s4aeURey5LLy",
    "outputId": "8a3a62c6-852e-48ae-ab5c-9d3b86eaf082",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Total number of samples in test folder\")\n!find test_mini -type d -or -type f -printf '.' | wc -c",
   "metadata": {
    "_uuid": "9560f0dd98c3decf8fceed05072def71efc2462e",
    "id": "LBLq_gYD5LOm",
    "outputId": "7af18bcd-a0e3-437d-c26b-b83e726d48b4",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Fine tune Inception Pretrained model using Food 101 dataset**",
   "metadata": {
    "_uuid": "3a9e9afbfe1fc303c23c06c27444b66988ab8e9d",
    "id": "upx61ukJiA8B"
   }
  },
  {
   "cell_type": "code",
   "source": "K.clear_session()\nn_classes = 3\nimg_width, img_height = 299, 299\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\nnb_train_samples = 2250 #75750\nnb_validation_samples = 750 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_3class.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples // batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples // batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_3class.hdf5')\n",
   "metadata": {
    "_uuid": "8d08ece78ab731f7ac8a9b4b581e42e29093bcca",
    "id": "JBs1U7hZkp1U",
    "outputId": "83b079b8-2550-41db-fcac-961fb11c07fc",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class_map_3 = train_generator.class_indices\nclass_map_3",
   "metadata": {
    "_uuid": "558180f917fba895b9792328b234421c097f1ba0",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Visualize the accuracy and loss plots**",
   "metadata": {
    "_uuid": "732521c21ce35b5be0d571cbe9d392d93755671c",
    "id": "KbDzLAHGpJXQ"
   }
  },
  {
   "cell_type": "code",
   "source": "def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n",
   "metadata": {
    "_uuid": "27084e072edb2c961bfa6be87d80f273d32533c2",
    "id": "SjRm_AWZpPZm",
    "outputId": "42a0123b-d15d-4ffa-cba4-e92cd81b0324",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plot_accuracy(history,'FOOD101-Inceptionv3')\nplot_loss(history,'FOOD101-Inceptionv3')",
   "metadata": {
    "_uuid": "a73c830d2fbd841c9541628061313e4fd8506a51",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Predicting classes for new images from internet using the best trained model**",
   "metadata": {
    "_uuid": "16347094046271c17a55125f5ff6bc002edc6152",
    "id": "qjeWMHrCwSoS"
   }
  },
  {
   "cell_type": "code",
   "source": "%%time\n# Loading the best saved model to make predictions\nK.clear_session()\nmodel_best = load_model('best_model_3class.hdf5',compile = False)",
   "metadata": {
    "_uuid": "dead5f30514dd02e80463adb162ca508ca34ee74",
    "id": "XBb-sj73pNc7",
    "outputId": "1bfbd6fa-3785-443a-989b-ebd0069613b4",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def predict_class(model, images, show = True):\n  for img in images:\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img /= 255.                                      \n\n    pred = model.predict(img)\n    index = np.argmax(pred)\n    food_list.sort()\n    pred_value = food_list[index]\n    if show:\n        plt.imshow(img[0])                           \n        plt.axis('off')\n        plt.title(pred_value)\n        plt.show()\n        print(pred_value)\n        \n    api_url = 'https://api.calorieninjas.com/v1/nutrition?query='\n    query = pred_value.replace(\"_\",\" \")\n    response = requests.get(api_url + query, headers={'X-Api-Key': 's9S3fiY9BWYZtpCebS71Rg==Qk1nbauJKhKE9k8R'})\n    if response.status_code == requests.codes.ok:\n        print(response.text)\n    else:\n        print(\"Error:\", response.status_code, response.text)\n        \n    ",
   "metadata": {
    "_uuid": "310fae2c5342c7f432962c4b2e90f09724b5d2c2",
    "id": "5MIBtyj1pFoK",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Downloading images from internet using the URLs\n!wget -O samosa.jpg http://veggiefoodrecipes.com/wp-content/uploads/2016/05/lentil-samosa-recipe-01.jpg\n!wget -O applepie.jpg https://acleanbake.com/wp-content/uploads/2017/10/Paleo-Apple-Pie-with-Crumb-Topping-gluten-free-grain-free-dairy-free-15.jpg\n!wget -O pizza.jpg https://media.glassdoor.com/l/78/17/b3/69/pizza.jpg\n!wget -O omelette.jpg https://www.seriouseats.com/2020/06/20200602-western-denver-omelette-daniel-gritzer-8.jpg\n\n# If you have an image in your local computer and want to try it, uncomment the below code to upload the image files\n\n# from google.colab import files\n# image = files.upload()",
   "metadata": {
    "_uuid": "19b36af37ba3c10e01e7d7b341a1e9482a48980d",
    "id": "4G3dZtmwJNuX",
    "outputId": "e8cc685a-8037-4cfe-c5d3-3dc908acf271",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Make a list of downloaded images and test the trained model\nimages = []\nimages.append('applepie.jpg')\nimages.append('pizza.jpg')\nimages.append('omelette.jpg')\nimages.append('samosa.jpg')\npredict_class(model_best, images, True)\n\n\n\n\n",
   "metadata": {
    "_uuid": "75bf083ef761963365a06ed543ac4c33d048ba52",
    "id": "uzLVocClxD0f",
    "outputId": "c8b03921-5cb7-4a37-be70-0351d466d0c5",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Fine tune Inceptionv3 model with 11 classes of data**",
   "metadata": {
    "_uuid": "d36d6ce06a0de7243311e17b866d5e60c4f7641b",
    "id": "rdaEgO6F7SBK"
   }
  },
  {
   "cell_type": "code",
   "source": "# Helper function to select n random food classes\ndef pick_n_random_classes(n):\n  food_list = []\n  random_food_indices = random.sample(range(len(foods_sorted)),n) # We are picking n random food classes\n  for i in random_food_indices:\n    food_list.append(foods_sorted[i])\n  food_list.sort()\n  return food_list\n  ",
   "metadata": {
    "_uuid": "43528bc9794091be03f6d4480a8a6a14ce58a1eb",
    "id": "EUe06wqASS1s",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Lets try with more classes than just 3. Also, this time lets randomly pick the food classes\nn = 11\nfood_list = pick_n_random_classes(n)\nfood_list = ['apple_pie', 'beef_carpaccio', 'bibimbap', 'cup_cakes', 'foie_gras', 'french_fries', 'garlic_bread', 'pizza', 'spring_rolls', 'spaghetti_carbonara', 'strawberry_shortcake']\nprint(\"These are the randomly picked food classes we will be training the model on...\\n\", food_list)",
   "metadata": {
    "_uuid": "b3088cb98adad92a14b1c8e656991ce9e2854726",
    "id": "b1DiwFi3SSrV",
    "outputId": "b7ea91c2-7112-4cc7-8b0a-fe65aa7c15cc",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create the new data subset of n classes\nprint(\"Creating training data folder with new classes...\")\ndataset_mini(food_list, src_train, dest_train)",
   "metadata": {
    "_uuid": "adfc9bddef628878eb90259c2bdaf2ce3411b832",
    "id": "eyfKi0kqZQVo",
    "outputId": "73add364-90c4-4331-9a2a-570e6e49ac27",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Total number of samples in train folder\")\n!find train_mini -type d -or -type f -printf '.' | wc -c",
   "metadata": {
    "_uuid": "8ac366a77d7dd10f46a34bd9c48b63d308e756af",
    "id": "_qdfhtWQZQeW",
    "outputId": "e9bcd0ea-4dd5-483d-fbfe-bdf2c02fb963",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Creating test data folder with new classes\")\ndataset_mini(food_list, src_test, dest_test)",
   "metadata": {
    "_uuid": "353e712c762462d2051c86ed0a4b9ae54303ca23",
    "id": "kVyZYEVyZQm4",
    "outputId": "4e60cc20-7369-4aec-93d4-6e199a75d23c",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Total number of samples in test folder\")\n!find test_mini -type d -or -type f -printf '.' | wc -c",
   "metadata": {
    "_uuid": "ed15e0d8e8cdcd29f4c264883322196b797d9084",
    "id": "_hP14tE0ZQvH",
    "outputId": "3340d81d-89c1-4b3b-ecfd-975519dbb663",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Let's use a pretrained Inceptionv3 model on subset of data with 11 food classes\nK.clear_session()\n\nn_classes = n\nimg_width, img_height = 299, 299\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\nnb_train_samples = 8250 #75750\nnb_validation_samples = 2750 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(n,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_11class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_11class.log')\n\nhistory_11class = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples // batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples // batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_11class.hdf5')\n",
   "metadata": {
    "_uuid": "b94f4380c22c8118e9589f6f5dcdba84fe0484b4",
    "id": "SyoYObOKZQ4j",
    "outputId": "2d8edacf-739d-4562-e367-8339d1f01114",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class_map_11 = train_generator.class_indices\nclass_map_11",
   "metadata": {
    "_uuid": "6280805e09902ca1b11772a2d5c942d2edcca5d4",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plot_accuracy(history_11class,'FOOD101-Inceptionv3')\nplot_loss(history_11class,'FOOD101-Inceptionv3')",
   "metadata": {
    "_uuid": "0bd7c9c273e27e7452a06fe54b1875741a1e033a",
    "id": "9j6jD5Ny9Mvh",
    "outputId": "c915a0ea-1784-4f86-b1d5-b5185a301481",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\n# Loading the best saved model to make predictions\nK.clear_session()\nmodel_best = load_model('best_model_11class.hdf5',compile = False)\n\n    ",
   "metadata": {
    "_uuid": "ef4ab1c2f8402dd1bf1af334e8bc97c9eec2e776",
    "id": "BAXYCwWF8TmY",
    "outputId": "6b012d0b-83a7-4d1b-be12-e1718dc1194c",
    "_kg_hide-output": false,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Downloading images from internet using the URLs\n!wget -O fries.jpg https://wallpapercave.com/wp/wp3031767.jpg\n!wget -O springrolls.jpg https://howtofeedaloon.com/wp-content/uploads/2016/02/bibimbap-1.jpg\n!wget -O pizza.jpg https://i.pinimg.com/originals/43/0f/83/430f83bfa304c69f4f6c96abbb38223e.jpg\n!wget -O garlicbread.jpg https://tableagent.s3.amazonaws.com/media/crumbs/xl/229_93.jpg\n\n# If you have an image in your local computer and want to tr\n\n\n# from google.colab import files\n# image = files.upload()",
   "metadata": {
    "_uuid": "199eafdf8dd36430f1bd1236e88745471dda2095",
    "id": "9jhn5e607xml",
    "outputId": "914c6fbe-a5ba-4e80-c976-1fcdf93e392c",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Make a list of downloaded images and test the trained model\nimages = []\nimages.append('fries.jpg')\nimages.append('pizza.jpg')\nimages.append('springrolls.jpg')\nimages.append('garlicbread.jpg')\npredict_class(model_best, images, True)",
   "metadata": {
    "_uuid": "f2be475f3d5bf5146f4c02950faa363a9c8a50ff",
    "id": "HAFb8xSj9Ygn",
    "outputId": "e8122914-ce09-41c0-b2bd-d53787d2c2ae",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Model Explainability**",
   "metadata": {
    "_uuid": "3e4712669a62fe3d94db12851a6db5dfe3721a4a"
   }
  },
  {
   "cell_type": "code",
   "source": "# Load the saved model trained with 3 classes\nK.clear_session()\nprint(\"Loading the model..\")\nmodel = load_model('best_model_3class.hdf5',compile = False)\nprint(\"Done!\")\n",
   "metadata": {
    "_uuid": "93d8f5442332a744f63e85254e4606ced953432b",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "* **Summary of the model gives us the list of all the layers in the network along with other useful details**",
   "metadata": {
    "_uuid": "970e958f3d2b852aafc8134bd3c999f0f9c27f0c"
   }
  },
  {
   "cell_type": "code",
   "source": "model.summary()",
   "metadata": {
    "_uuid": "aba498e59b48d5c754e4c9658d0265fdf4672ec3",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "* **Defining some helper functions**",
   "metadata": {
    "_uuid": "93c17b10ebd24b31ae7be2f36bf5905fbe6f3ac2"
   }
  },
  {
   "cell_type": "code",
   "source": "def deprocess_image(x):\n    # normalize tensor: center on 0., ensure std is 0.1\n    x -= x.mean()\n    x /= (x.std() + 1e-5)\n    x *= 0.1\n\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x *= 255\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n",
   "metadata": {
    "_uuid": "763f8eaa413fe08e4ca740b77f367a3ccfa8864e",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def generate_pattern(layer_name, filter_index, size=150):\n    # Build a loss function that maximizes the activation\n    # of the nth filter of the layer considered.\n    layer_output = model.get_layer(layer_name).output\n    loss = K.mean(layer_output[:, :, :, filter_index])\n\n    # Compute the gradient of the input picture wrt this loss\n    grads = K.gradients(loss, model.input)[0]\n\n    # Normalization trick: we normalize the gradient\n    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n\n    # This function returns the loss and grads given the input picture\n    iterate = K.function([model.input], [loss, grads])\n    \n    # We start from a gray image with some noise\n    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n\n    # Run gradient ascent for 40 steps\n    step = 1.\n    for i in range(40):\n        loss_value, grads_value = iterate([input_img_data])\n        input_img_data += grads_value * step\n        \n    img = input_img_data[0]\n    return deprocess_image(img)",
   "metadata": {
    "_uuid": "ac5fe6f82f0087295b81938da751bc3f6e90bc28",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_activations(img, model_activations):\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img /= 255. \n    plt.imshow(img[0])\n    plt.show()\n    return model_activations.predict(img)\n    ",
   "metadata": {
    "_uuid": "ce5c9c5a88e68022c5e90399de25124476d1109a",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def show_activations(activations, layer_names):\n    \n    images_per_row = 16\n\n    # Now let's display our feature maps\n    for layer_name, layer_activation in zip(layer_names, activations):\n        # This is the number of features in the feature map\n        n_features = layer_activation.shape[-1]\n\n        # The feature map has shape (1, size, size, n_features)\n        size = layer_activation.shape[1]\n\n        # We will tile the activation channels in this matrix\n        n_cols = n_features // images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n        # We'll tile each filter into this big horizontal grid\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0,\n                                                 :, :,\n                                                 col * images_per_row + row]\n                # Post-process the feature to make it visually palatable\n                channel_image -= channel_image.mean()\n                channel_image /= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size,\n                             row * size : (row + 1) * size] = channel_image\n\n        # Display the grid\n        scale = 1. / size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\n    plt.show()\n\n",
   "metadata": {
    "_uuid": "faa487b2c9aa71ed61acaba24e3bf5902d60dcb5",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "len(model.layers)",
   "metadata": {
    "_uuid": "4c26dfe3bc711db01646145bb4308021dfe4a45a",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# We start with index 1 instead of 0, as input layer is at index 0\nlayers = [layer.output for layer in model.layers[1:11]]\n# We now initialize a model which takes an input and outputs the above chosen layers\nactivations_output = models.Model(inputs=model.input, outputs=layers)",
   "metadata": {
    "_uuid": "d620cf1653800e476661eb6f39043b08efdb17a8",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**As seen below, the 10 chosen layers contain 3 convolution, 3 batch normalization, 3 activation and 1 max pooling layers**",
   "metadata": {
    "_uuid": "4621a88d9f09b5579b84fafda9f7beefccd46cc0"
   }
  },
  {
   "cell_type": "code",
   "source": "layers",
   "metadata": {
    "_uuid": "9ab2b5edbbde744049def8d1f82bc108b908b530",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Get the names of all the selected layers**",
   "metadata": {
    "_uuid": "6ef3ac62585ea825c4e1e3a789c15e8bb6e5a2a9"
   }
  },
  {
   "cell_type": "code",
   "source": "layer_names = []\nfor layer in model.layers[1:11]:\n    layer_names.append(layer.name)\nprint(layer_names)",
   "metadata": {
    "_uuid": "ec899b27f7042c4f5a01fd2111e9df4133a155bf",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Provide an input to the model and get the activations of all the 10 chosen layers**",
   "metadata": {
    "_uuid": "f0607820c01b7fd0ec1b7334218aff8cf80c7d0d"
   }
  },
  {
   "cell_type": "code",
   "source": "food = 'applepie.jpg'\nactivations = get_activations(food,activations_output)",
   "metadata": {
    "_uuid": "eb23311a44ef7f2ee4db3c015ed0d790fbc7c28e",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "* **activations** contain the outputs of all the 10 layers which can be plotted and visualized",
   "metadata": {
    "_uuid": "dd723f92e2fbed1ff7412105f512e42335ea58d7"
   }
  },
  {
   "cell_type": "markdown",
   "source": "**Visualize the activations of intermediate layers from layer 1 to 10**",
   "metadata": {
    "_uuid": "851bf660e561db76f46ed5c6b25aeaafffb5f7d7"
   }
  },
  {
   "cell_type": "code",
   "source": "show_activations(activations, layer_names)",
   "metadata": {
    "_uuid": "57e595c6d6ec1862f58829a214105f94e234bd2c",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Get the activations for a different input / food**",
   "metadata": {
    "_uuid": "78a622fb2ba644f8d60c7905f1e14c8d6542be4d"
   }
  },
  {
   "cell_type": "code",
   "source": "food = 'pizza.jpg'\nactivations = get_activations(food,activations_output)",
   "metadata": {
    "_uuid": "d30e9ea4b9b0ecb4fe1a40d971217d9152e278e5",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "show_activations(activations, layer_names)",
   "metadata": {
    "_uuid": "929114553760fb3d2e1c8259c65700f1b879e535",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Look into the sparse activations in the layer activation_1**",
   "metadata": {
    "_uuid": "da0296b7ac03e2c7d725a8d6cd9a3a775a93470a"
   }
  },
  {
   "cell_type": "code",
   "source": "# Get the index of activation_1 layer which has sparse activations\nind = layer_names.index('activation_1')\nsparse_activation = activations[ind]\na = sparse_activation[0, :, :, 13]\na",
   "metadata": {
    "_uuid": "ca1fc478ca3f4f8c5a45c8674194860de31648af",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "all (np.isnan(a[j][k])  for j in range(a.shape[0]) for k in range(a.shape[1]))",
   "metadata": {
    "_uuid": "c7872185d48380e88e4432e3cdabac26681d6d88",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Get the index of batch_normalization_1 layer which has sparse activations\nind = layer_names.index('batch_normalization_1')\nsparse_activation = activations[ind]\nb = sparse_activation[0, :, :, 13]\nb",
   "metadata": {
    "_uuid": "7d36fe148ec69c70f9db5e294bd12b8ac5743c05",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Show the activation outputs of 1st, 2nd and 3rd Conv2D layer activations to compare how layers get abstract with depth**",
   "metadata": {
    "_uuid": "83f536f8aac24c1e6046f6392ee0ee68d1bc5028"
   }
  },
  {
   "cell_type": "code",
   "source": "first_convlayer_activation = activations[0]\nsecond_convlayer_activation = activations[3]\nthird_convlayer_activation = activations[6]\nf,ax = plt.subplots(1,3, figsize=(10,10))\nax[0].imshow(first_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[0].axis('OFF')\nax[0].set_title('Conv2d_1')\nax[1].imshow(second_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[1].axis('OFF')\nax[1].set_title('Conv2d_2')\nax[2].imshow(third_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[2].axis('OFF')\nax[2].set_title('Conv2d_3')\n",
   "metadata": {
    "_uuid": "2aa73920abb8c4e96b3f9f1d49c5bc9b3e3384f7",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_attribution(food):\n    img = image.load_img(food, target_size=(299, 299))\n    img = image.img_to_array(img) \n    img /= 255. \n    f,ax = plt.subplots(1,3, figsize=(15,15))\n    ax[0].imshow(img)\n    \n    img = np.expand_dims(img, axis=0) \n    \n    preds = model.predict(img)\n    class_id = np.argmax(preds[0])\n    ax[0].set_title(\"Input Image\")\n    class_output = model.output[:, class_id]\n    last_conv_layer = model.get_layer(\"mixed10\")\n    \n    grads = K.gradients(class_output, last_conv_layer.output)[0]\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img])\n    for i in range(2048):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n    \n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n    ax[1].imshow(heatmap)\n    ax[1].set_title(\"Heat map\")\n    \n    \n    act_img = cv2.imread(food)\n    heatmap = cv2.resize(heatmap, (act_img.shape[1], act_img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed = cv2.addWeighted(act_img, 0.6, heatmap, 0.4, 0)\n    cv2.imwrite('classactivation.png', superimposed)\n    img_act = image.load_img('classactivation.png', target_size=(299, 299))\n    ax[2].imshow(img_act)\n    ax[2].set_title(\"Class Activation\")\n    plt.show()\n    return preds",
   "metadata": {
    "_uuid": "52251160e5b3c0a52f71ff48db78d1d24982ba9a",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Showing the class map..\")\nprint(class_map_3)",
   "metadata": {
    "_uuid": "9b24d222ab15dccc36f58d048cdd450531780b43",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred = get_attribution('applepie.jpg')\nprint(\"Here are softmax predictions..\",pred)",
   "metadata": {
    "_uuid": "e67af6ec477f5c1c552eabf2a279a7a4d0321542",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "###  **See how the class activation map looks for a different image**",
   "metadata": {
    "_uuid": "ba1376f87f8921015cd6a9c2cee1f2a31603ff7b"
   }
  },
  {
   "cell_type": "code",
   "source": "pred = get_attribution('pizza.jpg')\nprint(\"Here are softmax predictions..\",pred)",
   "metadata": {
    "_uuid": "7703de1adbb3c7c4d36a156255cac7544a753ec9",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Lets see if we can break the model or see what it does when we surpise it with different data!**",
   "metadata": {
    "_uuid": "50d3b1b85f1e1521f241f9da8a66337b4c2b4081"
   }
  },
  {
   "cell_type": "code",
   "source": "!wget -O piepizza.jpg https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/piepizza.jpg\n!wget -O piepizzas.png https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/piepizzas.png\n!wget -O pizzapie.jpg https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/pizzapie.jpg\n!wget -O pizzapies.png https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/pizzapies.png    ",
   "metadata": {
    "_uuid": "3f6b15f42226445649a7efde851c3c34cfd12976",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "food = 'piepizza.jpg'\nactivations = get_activations(food,activations_output)",
   "metadata": {
    "_uuid": "3bd18ee85715a2c5ea93ac42c90f5013371e9606",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "show_activations(activations, layer_names)",
   "metadata": {
    "_uuid": "3002cc239b1e6cc18d87a5cf68422f779a5feb4d",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred = get_attribution('piepizza.jpg')\nprint(\"Here are softmax predictions..\",pred)",
   "metadata": {
    "_uuid": "7d586ebc28865f330533978285cdb86ea0b034e4",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "food = 'pizzapie.jpg'\nactivations = get_activations(food,activations_output)",
   "metadata": {
    "_uuid": "5ee84660d1c611e26210c8fdd039eed0582f9c17",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred = get_attribution('pizzapie.jpg')\nprint(\"Here are softmax predictions..\",pred)",
   "metadata": {
    "_uuid": "2dea63dad490095884d23d17ba3408a783ef7221",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **More surprise data to the model...**",
   "metadata": {
    "_uuid": "486e22e0a40bea49f7c2bffca914cee10bce049a"
   }
  },
  {
   "cell_type": "code",
   "source": "food = 'pizzapies.png'\nactivations = get_activations(food,activations_output)",
   "metadata": {
    "_uuid": "94f1df50e0160a18fd4fc8be1415f53ffa5aa4d1",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred = get_attribution('pizzapies.png')\nprint(\"Here are softmax predictions..\",pred)",
   "metadata": {
    "_uuid": "30039ce99f4d49bb562cb08014a98691ec1d1ef0",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "food = 'piepizzas.png'\nactivations = get_activations(food,activations_output)",
   "metadata": {
    "_uuid": "f4215643fc7ea4ea01003105c36d75ac8643f636",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred = get_attribution('piepizzas.png')\nprint(\"Here are softmax predictions..\",pred)",
   "metadata": {
    "_uuid": "ea41b1a1ee4f286d736a7b582151e15619ca4666",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
